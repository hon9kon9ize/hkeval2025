陳述 1| 線性回歸估計量喺所有無偏估計量中具有最小方差。陳述 2| AdaBoost 組合嘅分類器所分配嘅系數 α 永遠係非負。,真，真,假，假,真，假,假，真,D
陳述 1| RoBERTa 喺一個大約係 BERT 預訓練語料庫 10 倍大嘅語料庫上進行預訓練。陳述 2| 2018 年嘅 ResNeXts 通常使用 tanh 激活函數。,真，真,假，假,真，假,假，真,C
陳述 1| 支持向量機，好似邏輯迴歸模型咁，會根據輸入範例畀一個可能標籤嘅機率分佈。陳述 2| 當我哋由線性核轉去高階多項式核時，我哋預期支持向量整體上會保持不變。,真，真,假，假,真，假,假，真,B
一個機器學習問題涉及四個屬性加上一個類別。每個屬性都有 3、2、2 和 2 個可能值。該類別有 3 個可能值。有多少個最大的可能不同範例？,12,24,48,72,D
截至 2020 年，邊種架構最適合分類高解像度圖像？,卷積神經網絡,圖神經網絡,全連接神經網絡,徑向基函數神經網絡,A
"陳述 1| 數據嘅對數似然性會喺期望最大化演算法嘅每次迭代中增加。
陳述 2| Q 學習嘅一個缺點係，佢只可以用喺學習者事先知道佢嘅行為點樣影響佢嘅環境嘅情況。",真，真,假，假,真，假,假，真,B
假設我哋已經計咗我哋成本函數嘅梯度並將佢儲喺一個向量 g 入面。喺畀咗梯度嘅情況下，一次梯度下降更新嘅成本係幾多？,O(D),O(N),O(ND),O(ND^2),A
"陳述 1｜對於連續隨機變數 x 及其機率分佈函數 p(x)，對於所有 x，0 ≤ p(x) ≤ 1。
陳述 2｜決策樹係透過最小化資訊增益嚟學習。",真，真,假，假,真，假,假，真,B
考慮以下嘅貝葉斯網絡。呢個貝葉斯網絡 H -> U <- P <- W 需要幾多個獨立參數？,2,4,8,16,C
當訓練範例嘅數量趨向無限時，用嗰啲數據訓練嘅模型將會：,方差較低,方差較高,方差相同,以上皆非,A
"陳述 1| 2D 平面中所有矩形（包括非軸對齊矩形）的集合可以粉碎一組 5 個點。
陳述 2| 當 k = 1 時，k 最近鄰分類器的 VC 維度為無窮大。",真，真,假，假,真，假,假，真,A
_ 指一個既唔能夠模擬訓練數據又唔能夠推廣到新數據嘅模型。,良好擬合,過度擬合,欠擬合,以上全部,C
陳述 1| F1 分數對於類別高度不平衡嘅數據集特別有用。陳述 2| ROC 曲線下面積係用嚟評估異常檢測器嘅主要指標之一。,真，真,假，假,真，假,假，真,A
陳述 1| 反向傳播演算法學習一個有隱藏層嘅全局最優神經網絡。陳述 2| 一條線嘅 VC 維度最多係 2，因為我可以搵到至少一個有 3 個點嘅情況，係任何線都唔可以將佢哋分開。,真，真,假，假,真，假,假，真,B
高熵表示分類中嘅分區係,純,唔純,有用,冇用,B
陳述 1| 喺最初嘅 ResNet 論文入面用咗 Layer Normalization，而唔係 Batch Normalization。陳述 2| DCGAN 用自注意力嚟穩定訓練。,真，真,假，假,真，假,假，真,B
喺為一個特定數據集建立線性回歸模型時，你觀察到其中一個特徵嘅系數有一個相對較高嘅負值。呢個表明,呢個特徵對模型有強烈影響（應該保留）,呢個特徵對模型冇強烈影響（應該忽略）,喺冇額外資訊嘅情況下，唔能夠評論呢個特徵嘅重要性,乜都唔能夠確定。,C
對於一個神經網絡，以下邊個結構假設對欠擬合（即係一個高偏差模型）同過擬合（即係一個高變異模型）之間嘅權衡影響最大：,隱藏節點嘅數量,學習率,權重嘅初始選擇,使用一個常數項單位輸入,A
對於多項式迴歸，以下邊個結構假設對欠擬合同過擬合之間嘅取捨影響最大：,多項式次數,我哋係通過矩陣求逆或梯度下降來學習權重,假設嘅高斯噪聲嘅方差,使用常數項單位輸入,A
陳述 1|截至 2020 年，某些模型喺 CIFAR-10 上達到超過 98% 嘅準確度。陳述 2|最初嘅 ResNets 唔係用 Adam 優化器進行優化。,真，真,假，假,真，假,假，真,A
K-means 演算法：,要求特徵空間嘅維度唔可以大過樣本數,當 K = 1 時有最細嘅目標函數值,為指定嘅簇數最小化類內方差,當且僅當初始均值被選為部分樣本本身時收斂到全局最優,C
"陳述 1| VGGNets 嘅卷積核比 AlexNet 第一層嘅核闊度同高度細。
陳述 2| 喺批次正規化之前已經引入咗依賴數據嘅權重初始化程序。",真，真,假，假,真，假,假，真,A
"以下矩陣嘅秩係幾多？A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]",0,1,2,3,B
陳述 1| 密度估計（例如使用核密度估計器）可以用嚟執行分類。陳述 2| 邏輯迴歸同高斯樸素貝葉斯（具有同一類協方差）之間嘅對應關係表示兩個分類器嘅參數之間存在一對一嘅對應關係。,真，真,假，假,真，假,假，真,C
假設我哋想對空間數據進行聚類，例如屋企嘅幾何位置。我哋希望產生唔同大細同形狀嘅聚類。以下邊種方法最合適？,決策樹,基於密度嘅聚類,基於模型嘅聚類,K-means 聚類,B
陳述 1|喺 AdaBoost 入面，錯誤分類嘅例子嘅權重會以相同嘅乘法因子增加。陳述 2|喺 AdaBoost 入面，加權訓練誤差 e_t，即第 t 個弱分類器喺權重為 D_t 嘅訓練數據上嘅訓練誤差，會隨著 t 而增加。,真，真,假，假,真，假,假，真,A
MLE 估計通常唔理想係因為,佢哋有偏差,佢哋有高變異,佢哋唔係一致估計量,以上皆非,B
梯度下降嘅計算複雜度係，,線性喺 D,線性喺 N,多項式喺 D,取決於迭代次數,C
平均多棵決策樹嘅輸出有助於,增加偏差,減少偏差,增加變異,減少變異,D
喺識別到嘅特徵子集上應用線性回歸得到嘅模型可能同喺識別子集過程結束時得到嘅模型唔同，呢個係,最佳子集選擇,向前逐步選擇,向前階段性選擇,以上全部,C
神經網絡：,優化凸目標函數,只能用隨機梯度下降法訓練,可以使用不同激活函數的組合,以上皆非,C
話疾病 D 嘅發生率係每 100 人大約 5 宗（即係 P(D) = 0.05）。設布林隨機變數 D 表示病人「患有疾病 D」，設布林隨機變數 TP 表示「測試呈陽性」。已知疾病 D 嘅測試非常準確，即係患有疾病時測試呈陽性嘅概率係 0.99，而冇患有疾病時測試呈陰性嘅概率係 0.97。測試呈陽性嘅先驗概率 P(TP) 係幾多？,0.0368,0.473,0.078,以上皆非,C
"陳述 1| 喺通過徑向基核函數映射到特徵空間 Q 之後，使用非加權歐幾里得距離嘅 1-NN 可能能夠喺原始空間中獲得比喺原始空間中更好的分類性能（儘管我哋無法保證這一點）。
陳述 2| 感知器的 VC 維度小於簡單線性 SVM 的 VC 維度。",真，真,假，假,真，假,假，真,B
網格搜尋嘅缺點係,佢唔可以應用於不可微嘅函數。,佢唔可以應用於不連續嘅函數。,佢好難實作。,佢喺多重線性回歸中運行得相當慢。,D
根據各種線索預測一個地區嘅降雨量係一個 ______ 問題。,有監督學習,無監督學習,聚類,以上皆非,A
以下邊個講法關於迴歸係錯？,佢將輸入同輸出聯繫埋一齊。,佢用嚟預測。,佢可以用嚟解釋。,佢發現因果關係,D
以下邊個係修剪決策樹嘅主要原因？,喺測試期間慳返運算時間,慳返儲存決策樹嘅空間,令訓練集嘅誤差變細,避免過度擬合訓練集,D
陳述 1| 核密度估計等同於喺原始數據集嘅每個點 Xi 執行核回歸，值 Yi = 1/n。陳述 2| 一棵已學習嘅決策樹嘅深度可以大於用嚟建立棵樹嘅訓練範例數量。,真，真,假，假,真，假,假，真,B
假設你嘅模型過度擬合。以下邊個唔係一個嘗試減少過度擬合嘅有效方法？,增加訓練數據量。,改善用嚟減少誤差嘅優化演算法。,減少模型複雜度。,減少訓練數據嘅雜訊。,B
"陳述 1| softmax 函數通常用於多類別邏輯迴歸。
陳述 2| 非均勻 softmax 分佈嘅溫度會影響佢嘅熵。",真，真,假，假,真，假,假，真,A
以下邊個講法關於 SVM 係真？,對於一個二維數據點，一個線性 SVM 學到嘅分隔超平面會係一條直線。,理論上，一個高斯核 SVM 唔能夠模擬任何複雜嘅分隔超平面。,對於一個 SVM 入面用嘅每個核函數，都可以得到一個等價嘅閉合形式基展開。,一個 SVM 入面嘅過度擬合唔關支持向量嘅數量事。,A
以下邊個係由貝葉斯網絡 H -> U <- P <- W 描述嘅 H、U、P 同 W 嘅聯合概率？[注意：係條件概率嘅乘積],"P(H, U, P, W) = P(H) * P(W) * P(P) * P(U)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(W | H, P)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P)",以上皆非,C
陳述 1| 由於具有徑向基核的 SVM 的 VC 維度是無限的，因此這樣的 SVM 必定比具有有限 VC 維度的多項式核的 SVM 差。陳述 2| 具有線性激活函數的兩層神經網絡本質上是線性分隔器的加權組合，在給定的數據集上訓練；建立在線性分隔器上的提升算法也找到線性分隔器的組合，因此這兩個算法將給出相同的結果。,真，真,假，假,真，假,假，真,B
陳述 1| ID3 演算法保證可以搵到最佳決策樹。陳述 2| 考慮一個密度函數 f() 喺任何地方都唔等於零嘅連續概率分佈。一個值 x 嘅概率等於 f(x)。,真，真,假，假,真，假,假，真,B
畀一個有 N 個輸入節點、無隱藏層、一個輸出節點、用熵損失同 Sigmoid 激活函數嘅神經網絡，以下邊個演算法（用適當嘅超參數同初始化）可以用嚟搵到全局最優？,隨機梯度下降,迷你批次梯度下降,批次梯度下降,以上全部,D
喺一個線性模型入面加入更多基底函數，揀最有可能嘅選項：,減少模型偏差,減少估計偏差,減少變異,唔影響偏差同變異,A
考慮以下嘅貝葉斯網絡。如果我哋冇假設獨立性或條件獨立性 H -> U <- P <- W，我哋需要幾多個獨立參數？,3,4,7,15,D
離群值偵測嘅另一個術語係？,異常偵測,單類偵測,訓練測試唔匹配嘅穩健性,背景偵測,A
"陳述 1| 我哋通過提升弱學習器 h 來學習分類器 f。f 的決策邊界的函數形式與 h 的相同，但參數不同。（例如，如果 h 是線性分類器，則 f 也是線性分類器）。
陳述 2| 交叉驗證可用於選擇提升中的迭代次數；此過程可能有助於減少過擬合。",真，真,假，假,真，假,假，真,D
陳述 1| 高速公路網絡喺 ResNets 之後引入，並且放棄最大池化而改用卷積。陳述 2| DenseNets 通常比 ResNets 耗用更多記憶體。,真，真,假，假,真，假,假，真,D
如果 N 係訓練數據集嘅實例數量，最近鄰居嘅分類運行時間係,O(1),O( N ),O(log N ),O( N^2 ),B
"陳述 1| 原始 ResNets 和 Transformers 係前饋神經網絡。
陳述 2| 原始 Transformers 使用自注意力，但原始 ResNet 唔使用。",真，真,假，假,真，假,假，真,A
陳述 1| RELU 唔係單調嘅，但 sigmoid 係單調嘅。陳述 2| 用梯度下降訓練嘅神經網絡好大機會收斂到全局最優。,真，真,假，假,真，假,假，真,D
一個神經網絡入面嘅 sigmoid 節點嘅數值輸出：,係無界嘅，包含所有實數。,係無界嘅，包含所有整數。,係界於 0 同 1 之間。,係界於 -1 同 1 之間。,C
以下邊個只可以用喺訓練數據係線性可分嘅時候？,線性硬邊距 SVM。,線性邏輯迴歸。,線性軟邊距 SVM。,質心法。,A
以下邊啲係空間聚類演算法？,基於分割嘅聚類,K-means 聚類,基於網格嘅聚類,以上全部,D
陳述 1| 支持向量機構造嘅最大邊際決策邊界喺所有線性分類器中具有最低嘅泛化誤差。陳述 2| 任何一個我哋從具有類條件高斯分佈嘅生成模型中得到嘅決策邊界，原則上都可以用 SVM 和度數小於或等於 3 嘅多項式核來複製。,真，真,假，假,真，假,假，真,D
"陳述 1| 線性模型嘅 L2 正則化傾向令模型比 L1 正則化更稀疏。
陳述 2| 殘差連接可以喺 ResNets 同 Transformers 度搵到。",真，真,假，假,真，假,假，真,D
"假設我哋想計算 P(H|E, F) 而我哋冇條件獨立嘅資訊。以下邊組數字足夠用嚟計算？","P(E, F), P(H), P(E|H), P(F|H)","P(E, F), P(H), P(E, F|H)","P(H), P(E|H), P(F|H)","P(E, F), P(E|H), P(F|H)",B
喺我哋進行 bagging 時，以下邊樣可以防止過度擬合？,使用有放回抽樣作為抽樣技術,使用弱分類器,使用唔容易過度擬合嘅分類演算法,對每個訓練嘅分類器執行驗證嘅做法,B
陳述 1| PCA 同 Spectral Clustering（例如 Andrew Ng 嘅）對兩個唔同嘅矩陣進行特徵分解。不過，呢兩個矩陣嘅大小係一樣。陳述 2| 由於分類係迴歸嘅一個特例，邏輯迴歸係線性迴歸嘅一個特例。,真，真,假，假,真，假,假，真,B
陳述 1| 斯坦福情緒樹庫包含電影評論，唔係書評。陳述 2| 賓夕法尼亞樹庫已經用於語言建模。,真，真,假，假,真，假,假，真,A
"以下矩陣嘅零空間嘅維度係幾多？A = [[3, 2, −9], [−6, −4, 18], [12, 8, −36]]",0,1,2,3,C
支持向量係乜嘢？,最遠離決策邊界嘅例子。,喺 SVM 入面計算 f(x) 所需嘅唯一例子。,數據質心。,喺 SVM 入面具有非零權重 αk 嘅所有例子。,B
陳述 1| Word2Vec 參數唔係用受限玻爾茲曼機初始化。陳述 2| tanh 函數係一個非線性激活函數。,真，真,假，假,真，假,假，真,A
如果你嘅訓練損失隨住 epoch 數量增加，以下邊個可能係學習過程嘅一個問題？,正則化太低，模型過度擬合,正則化太高，模型欠擬合,步長太大,步長太細,C
話疾病 D 嘅發生率係每 100 人大約 5 宗個案（即係 P(D) = 0.05）。設布林隨機變量 D 表示一個病人「患有疾病 D」，設布林隨機變量 TP 表示「測試呈陽性」。已知疾病 D 嘅測試非常準確，即係當你患有疾病時測試呈陽性嘅機率係 0.99，而當你冇患有疾病時測試呈陰性嘅機率係 0.97。當測試呈陽性時，你患有疾病 D 嘅後驗機率 P(D | TP) 係幾多？,0.0495,0.078,0.635,0.97,C
"陳述 1| 傳統機器學習結果假設訓練集和測試集是獨立且同分布的。
陳述 2| 2017 年，COCO 模型通常在 ImageNet 上進行預訓練。",真，真,假，假,真，假,假，真,A
"陳述 1| 喺同一個訓練集上，兩個唔同嘅核函數 K1(x, x0) 同 K2(x, x0) 得到嘅邊際值唔會話俾我哋知邊個分類器喺測試集上表現會比較好。
陳述 2| BERT 嘅激活函數係 GELU。",真，真,假，假,真，假,假，真,A
以下邊個係機器學習入面嘅聚類演算法？,期望最大化,CART,高斯樸素貝氏,Apriori,A
你啱啱完成訓練一個用嚟分類垃圾郵件嘅決策樹，但佢喺你嘅訓練集同測試集都表現得異常咁差。你知你嘅實作冇 bug，咁咩可能係造成呢個問題？,你嘅決策樹太淺。,你需要增加學習率。,你過度擬合。,以上皆非。,A
K 折交叉驗證係,線性 K,二次 K,三次 K,指數 K,A
"陳述 1｜工業規模嘅神經網絡通常喺 CPU 而唔係 GPU 上面訓練。
陳述 2｜ResNet-50 模型有超過 10 億個參數。",真，真,假，假,真，假,假，真,B
畀兩個布林隨機變量 A 同 B，其中 P(A) = 1/2、P(B) = 1/3、同埋 P(A | ¬B) = 1/4，咁 P(A | B) 係幾多？,1/6,1/4,3/4,1,D
人工智能帶嚟嘅生存風險最常同邊個教授聯繫埋一齊？,Nando de Frietas,Yann LeCun,Stuart Russell,Jitendra Malik,C
"陳述 1| 最大化邏輯迴歸模型嘅似然性會產生多個局部最優值。
陳述 2| 如果數據嘅分佈係已知嘅，冇任何分類器可以做得比樸素貝葉斯分類器更好。",真，真,假，假,真，假,假，真,B
對於核回歸，以下邊個結構假設對欠擬合同過擬合之間嘅取捨影響最大？,核函數係高斯函數、三角形函數定盒形函數,我哋用歐幾里得度量、L1 度量定 L∞ 度量,核寬度,核函數嘅最大高度,C
陳述 1| SVM 學習演算法保證可以搵到佢物件函數嘅全局最優假設。陳述 2| 透過徑向基核函數映射到特徵空間 Q 之後，一個感知器可能可以喺佢嘅原始空間獲得比佢嘅原始空間更好嘅分類性能（儘管我哋唔能夠保證呢一點）。,真，真,假，假,真，假,假，真,A
對於一個高斯貝葉斯分類器，以下邊個結構假設對欠擬合同過擬合之間嘅權衡影響最大：,我哋係用最大似然法定學習類別中心定梯度下降法,我哋係假設完全類別共變異數矩陣定對角類別共變異數矩陣,我哋係有相等類別先驗定從數據估計嘅先驗,我哋係容許類別有唔同嘅平均向量定我哋強迫佢哋共享同一個平均向量,B
"陳述 1| 當訓練數據集較少時，較容易發生過度擬合。
陳述 2| 當假設空間較少時，較容易發生過度擬合。",真，真,假，假,真，假,假，真,D
陳述 1 | 除了 EM，梯度下降法可以用嚟對高斯混合模型進行推論或學習。陳述 2 | 假設屬性數量固定，一個基於高斯嘅貝葉斯最優分類器可以喺時間上以線性方式學習數據集嘅記錄數量。,真，真,假，假,真，假,假，真,A
陳述 1|喺貝葉斯網絡中，連接樹演算法嘅推論結果同變量消除嘅推論結果係一樣。陳述 2|如果兩個隨機變量 X 同 Y 喺畀定另一個隨機變量 Z 嘅情況下係條件獨立，咁喺相應嘅貝葉斯網絡中，X 同 Y 嘅節點喺畀定 Z 嘅情況下係 d-分離。,真，真,假，假,真，假,假，真,C
畀咗一個由患有心臟病嘅病人嘅醫療紀錄組成嘅大型數據集，試吓了解下呢啲病人係咪可能會有唔同嘅群組，我哋可以為佢哋度身訂造唔同嘅治療方法。呢個係邊一種學習問題？,有監督學習,無監督學習,以上兩者（a）同（b）,以上兩者（a）同（b）都唔係,B
喺 PCA 入面要點做先可以得到同 SVD 一樣嘅投影？,將數據轉換為零均值,將數據轉換為零中位數,唔可能,以上皆非,A
"陳述 1| 1-最近鄰分類器嘅訓練誤差係 0。
陳述 2| 當數據點數量趨向無限大時，MAP 估計會接近所有可能先驗嘅 MLE 估計。換句話講，有足夠數據嘅話，先驗嘅選擇係唔重要。",真，真,假，假,真，假,假，真,C
喺做正則化嘅最小平方回歸時（假設最佳化可以完全做到），增加正則化參數 λ 嘅值會,永遠唔會減少訓練誤差。,永遠唔會增加訓練誤差。,永遠唔會減少測試誤差。,永遠唔會增加,A
以下邊個最能描述判別式方法嘗試建模乜嘢？（w 係模型入面嘅參數）,"p(y|x, w)","p(y, x)","p(w|x, w)",以上皆非,A
"陳述 1| 對於卷積神經網絡，CIFAR-10 分類效能可以超過 95%。
陳述 2| 神經網絡嘅集成唔會提升分類準確度，因為佢哋學習嘅表徵高度相關。",真，真,假，假,真，假,假，真,C
以下邊點係貝葉斯主義者同頻率主義者會唔同意嘅？,喺概率迴歸中使用非高斯噪聲模型。,喺迴歸中使用概率模型。,喺概率模型中對參數使用先驗分佈。,喺高斯判別分析中使用類別先驗。,C
陳述 1| BLEU 指標使用準確度，而 ROGUE 指標使用召回率。陳述 2| 隱藏馬可夫模型經常被用嚟建模英文句子。,真，真,假，假,真，假,假，真,A
"陳述 1| ImageNet 有唔同解像度嘅圖片。
陳述 2| Caltech-101 有多過 ImageNet 嘅圖片。",真，真,假，假,真，假,假，真,C
以下邊個更適合做特徵選擇？,Ridge,Lasso,兩者 (a) 和 (b) 都係,兩者 (a) 和 (b) 都唔係,B
假設你有一個 EM 演算法，可以搵到一個有潛在變量嘅模型嘅最大似然估計。你被要求修改演算法，令佢搵到 MAP 估計。你需要修改邊一個或邊啲步驟？,期望,最大化,唔需要修改,兩個都要,B
對於一個高斯貝葉斯分類器，以下邊個結構性假設最影響欠擬合同過擬合之間嘅取捨：,我哋係用最大似然法定係梯度下降法嚟學習類別中心,我哋係假設完全類別協方差矩陣定係對角線類別協方差矩陣,我哋係有相等嘅類別先驗定係從數據估計嘅先驗,我哋係容許類別有唔同嘅平均向量定係強迫佢哋共享同一個平均向量,B
"陳述 1| 對於任何兩個具有聯合分佈 p(x, y) 的變數 x 和 y，我們總是具有 H[x, y] ≥ H[x] + H[y]，其中 H 是熵函數。陳述 2| 對於某些有向圖，道德化會減少圖中存在的邊數。",真，真,假，假,真，假,假，真,B
以下邊個唔係監督式學習？,PCA,決策樹,線性回歸,樸素貝葉斯,A
陳述 1| 神經網絡嘅收斂取決於學習率。陳述 2| Dropout 將隨機選擇嘅激活值乘以零。,真，真,假，假,真，假,假，真,A
"以下邊個等於 P(A, B, C)，假設布林隨機變量 A、B 和 C，並且任何兩個變量之間都冇獨立性或條件獨立性假設？",P(A | B) * P(B | C) * P(C | A),"P(C | A, B) * P(A) * P(B)","P(A, B | C) * P(C)","P(A | B, C) * P(B | A, C) * P(C | A, B)",C
以下邊個任務最適合用聚類解決？,根據各種提示預測降雨量,偵測詐騙信用卡交易,訓練一個機械人解決迷宮,以上全部,B
喺線性回歸中應用正則化懲罰後，你發現 w 嘅某啲系數變咗做零。以下邊個懲罰可能被使用？,L0 範數,L1 範數,L2 範數,(a) 或 (b),D
"A 同 B 係兩個事件。如果 P(A, B) 減少而 P(A) 增加，以下邊個係真？",P(A|B) 減少,P(B|A) 減少,P(B) 減少,以上全部,B
陳述 1| 當為一組固定嘅觀察值學習 HMM 時，假設我哋唔知道隱藏狀態嘅真實數量（通常係咁），我哋可以通過允許更多隱藏狀態來增加訓練數據似然性。陳述 2| 協同過濾通常係建模用戶電影偏好嘅有用模型。,真，真,假，假,真，假,假，真,A
你緊係訓練緊一個線性回歸模型嚟做一個簡單嘅估計任務，而且留意到個模型過度擬合咗個數據。你決定加入 $\ell_2$ 正則化嚟懲罰權重。當你增加 $\ell_2$ 正則化系數時，個模型嘅偏差同方差會點樣變？,偏差增加；方差增加,偏差增加；方差減少,偏差減少；方差增加,偏差減少；方差減少,B
"邊個 PyTorch 1.8 指令會產生一個 $10\times 5$ 嘅高斯矩陣，每個元素都係由 $\mathcal{N}(\mu=5,\sigma^2=16)$ 獨立同分佈抽樣，同埋一個 $10\times 10$ 嘅均勻矩陣，每個元素都係由 $U[-1,1)$ 獨立同分佈抽樣？","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{torch.rand(10,10,low=-1,high=1)}","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{(torch.rand(10,10) - 0.5) / 0.5}","\texttt{5 + torch.randn(10,5) * 4} ; \texttt{2 * torch.rand(10,10) - 1}","\texttt{torch.normal(torch.ones(10,5)*5,torch.ones(5,5)*16)} ; \texttt{2 * torch.rand(10,10) - 1}",C
"陳述1| ReLU嘅梯度喺$x<0$時係零，而sigmoid梯度$\sigma(x)(1-\sigma(x))\le \frac{1}{4}$對於所有$x$。
陳述2| sigmoid有一個連續嘅梯度，而ReLU有一個唔連續嘅梯度。",真，真,假，假,真，假,假，真,A
以下邊個講法關於批次標準化係真？,應用批次標準化之後，該層嘅激活會跟隨標準高斯分佈。,如果一個批次標準化層緊接在仿射層之後，仿射層嘅偏差參數會變得多餘。,使用批次標準化時必須更改標準權重初始化。,對於卷積神經網絡，批次標準化等於層標準化。,B
假設我哋有以下目標函數：$\argmin_{w} \frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\gamma \norm{w}^2_2$ 咁 $\frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\lambda \norm{w}^2_2$ 關於 $w$ 嘅梯度係咩？,$\nabla_w f(w) = (X^\top X + \lambda I)w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + (\lambda+1) w$,C
以下邊個講法關於卷積核係真？,用 $\begin{bmatrix}1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$ 對一個影像做卷積唔會改變個影像,用 $\begin{bmatrix}0 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ 對一個影像做卷積唔會改變個影像,用 $\begin{bmatrix}1 & 1 & 1\\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}$ 對一個影像做卷積唔會改變個影像,用 $\begin{bmatrix}0 & 0 & 0\\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ 對一個影像做卷積唔會改變個影像,B
以下邊個係錯？,語義分割模型預測每個像素嘅類別，而多類別圖像分類器預測成幅圖像嘅類別。,一個 IoU（交集比並集）等於 $96\%$ 嘅邊界框好有可能會被視為真陽性。,當一個預測嘅邊界框唔對應場景入面嘅任何物件時，佢會被視為假陽性。,一個 IoU（交集比並集）等於 $3\%$ 嘅邊界框好有可能會被視為假陰性。,D
以下邊個係錯？,"以下呢個冇激活函數嘅全連接網絡係線性嘅：$g_3(g_2(g_1(x)))$, 其中 $g_i(x) = W_i x$ 而 $W_i$ 係矩陣。","Leaky ReLU $\max\{0.01x,x\}$ 係凸嘅。",ReLU 嘅組合例如 $ReLU(x) - ReLU(x-1)$ 係凸嘅。,損失 $\log \sigma(x)= -\log(1+e^{-x})$ 係凹嘅。,C
我哋用兩個隱藏層嘅全連接網絡嚟預測樓價。輸入係 $100$ 維，有幾個特徵，例如平方英尺數、家庭收入中位數等。第一個隱藏層有 $1000$ 個激活。第二個隱藏層有 $10$ 個激活。輸出係一個標量，代表樓價。假設一個香草網絡有仿射變換，並且沒有批次歸一化和激活函數中沒有可學習參數，這個網絡有多少個參數？,111021,110010,111110,110011,A
陳述 1| 函數 $\sigma(x)=(1+e^{-x})^{-1}$ 對 $x$ 嘅導數等於 $\text{Var}(B)$，其中 $B\sim \text{Bern}(\sigma(x))$ 係一個伯努利隨機變量。陳述 2| 將神經網絡每一層嘅偏差參數設為 0 會改變偏差-方差權衡，令模型嘅方差增加而模型嘅偏差減少。,真，真,假，假,真，假,假，真,C
